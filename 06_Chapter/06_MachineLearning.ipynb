{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# 06. 머신 러닝(Machine Learning) 개요\n",
    "***\n",
    "\n",
    "머신 러닝은 영상 처리, 번역기, 음성 인식, 스팸 메일 탐지 등 굉장히 다양한 분야에서 응용되고 있습니다.\n",
    "특히 머신 러닝의 한 갈래인 딥 러닝은 자연어 처리 엔지니어에게 필수 역량이 되어가고 있습니다.\n",
    "이번 챕터에서는 머신 러닝의 개념과 선형 회귀, 로지스틱 회귀, 소프트맥스 회귀와 같은 기본적인 모델을 이해합니다.\n",
    "그리고 이러한 이해를 바탕으로 다음 딥 러닝 챕터에서 기본적인 모델로부터 딥 러닝 모델로 개념을 확장해보겠습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eccf8379047f4acb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 06-01 머신 러닝이란(What is Machine Learning?)\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f5a382884492713"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 머신 러닝(Machine Learning)이 아닌 접근 방법의 한계\n",
    "***\n",
    "\n",
    "머신 러닝이 아닌 기존의 프로그래밍 작성 방식을 통해서는 해결하기 어려운 문제\n",
    "\n",
    "**예시 : 주어진 사진으로부터 고양이 사진인지 강아지 사진인지 판별하는 일.**\n",
    "\n",
    "사진을 보고 고양이 사진인지, 강아지 사진인지 판단하는 건 사람에게는 너무나 쉬운 일입니다.\n",
    "그런데 이 문제를 풀 수 있는 프로그램을 작성하는 것은 상당히 난해한 수준입니다.\n",
    "입력된 이미지로부터 강아지와 고양이를 구분할 수 있는 코드를 어떻게 작성할 수 있을까요?\n",
    "\n",
    "```python\n",
    "def prediction(이미지 as input):\n",
    "    어떻게 코딩해야하지?\n",
    "    return 결과\n",
    "```\n",
    "사진이란 건 사진을 보는 각도, 조명, 타겟의 변형(고양이의 자세)에 따라서 너무나 천차만별이라 사진으로부터 공통된 명확한 특징을 잡아내는 것이 쉽지 않다.\n",
    "사실, 결론을 미리 말씀드리면 해당 프로그램은 숫자를 정렬하는 것과 같은 명확한 알고리즘이 애초에 존재하지 않는다.\n",
    "\n",
    "사진으로부터 대상을 찾아내는 일은 사람이 규칙을 정의하는 것이 아니라 머신 러닝으로 문제를 해결하고 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "155703254b3062e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 머신 러닝 방식\n",
    "***\n",
    "\n",
    "![그림](img.png)\n",
    "\n",
    "\n",
    "머신 러닝이 위에서 언급한 예시 문제를 해결할 수 있는 이유는 해결을 위한 접근 방식이 기존의 프로그래밍 방식과는 다르기 때문이다.\n",
    "위 이미지에서 위쪽은 기존의 프로그래밍의 접근 방식, 아래쪽은 머신 러닝의 접근 방식을 보여준다.\n",
    "\n",
    "머신 러닝은 데이터가 주어지면, 기계가 스스로 데이터로부터 규칙성을 찾는 것에 집중한다.\n",
    "주어진 데이터로부터 규칙성을 찾는 과정을 우리는 훈련(training) 또는 학습(learning)이라고 한다.\n",
    "\n",
    "일단 규칙성을 발견하고나면, 그 후에 들어오는 새로운 데이터에 대해서 발견한 규칙성을 기준으로 정답을 찾아낸다.\n",
    "최근에는 머신 러닝의 한 갈래인 딥 러닝이 자연어 처리에서 굉장히 뛰어난 성능을 보여주고 있다.\n",
    "기계 번역기가 그러한데, 이러한 번역기는 사람이 직접 규칙을 정의해서 만드는 것보다 딥 러닝으로 모델이 스스로 규칙을 찾아내도록 구현하는 것이 훨씬 더 좋은 성능을 얻을 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb142fb74bfbd406"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 06-02 머신 러닝 훑어보기\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ae1586e69e30790"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 머신 러닝 모델의 평가\n",
    "***\n",
    "\n",
    "![그림](img_1.png)\n",
    "\n",
    "검증용 데이터는 모델의 성능을 평가하기 위한 용도가 아니라 모델의 성능을 조정하기 위한 용도\n",
    "모델이 훈련 데이터에 과적합(overfitting) 이 되고 있는지 판단하거나 하이퍼파라미터의 조정을 위한 용도\n",
    "\n",
    "- 하이퍼파라미터(초매개변수) : 모델의 성능에 영향을 주는 사람이 값을 지정하는 변수.(사용자가 직접 정해줄 수 있는 변수)\n",
    "- 매개변수 : 가중치와 편향. 학습을 하는 동안 값이 계속해서 변하는 수.(모델이 학습하는 과정에서 얻어지는 값 : ex. 가중지, 편향)\n",
    "\n",
    "훈련용 데이터로 훈련을 모두 시킨 모델은 검증용 데이터를 사용하여 정확도를 검증하며 하이퍼파라미터를 튜닝(tuning) 한다.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfb0c7c9f8cee305"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 분류(Classification)와 회귀(Regression)\n",
    "***\n",
    "전부는 아니지만 머신 러닝의 많은 문제는 분류 또는 회귀 문제에 속한다.\n",
    "이번 챕터에서는 선형 회귀(Lineare Regression)와 로지스틱 회귀(Logistic Regression)를 다룬다.\n",
    "선형 회귀는 대표적인 회귀 문제에 속하고, 로지스틱 회귀는 (이름은 회귀이지만) 대표적인 분류 문제에 속한다.\n",
    "\n",
    "분류는 또한 이진 분류(Binary Classification)과 다중 클래스 분류(Multi-Class Classification)로 나뉜다.\n",
    "엄밀히는 다중 레이블 분류(Multi-lable Classification)라는 또 다른 문제가 존재하지만, 이 책에서는 이진 분류와 다중 클래스 분류만을 다룬다.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dc4061e89c77276"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1) 이진 분류 문제(Binary Classification)\n",
    "주어진 입력에 대해서 두 개의 선택지 중 하나의 답을 선택해야 하는 경우\n",
    "(종합 시험 성적표를 보고 최종적으로 합격, 불합격인지 판단하는 문제, 메일을 보고나서 정상 메일, 스팸 메일인지를 판단하는 문제 등)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5591433a9d170cb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2) 다중 클래스 분류(Multi-class Classification)\n",
    "주어진 입력에 대해서 세 개 이상의 선택지 중에서 답을 선택해야 하는 경우\n",
    "(과학, 영어, IT, 학습지, 만화라는 레이블이 붙어있는 5개의 책장에 새 책 카테고리 분류) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b216bbe125a486b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3) 회귀 문제(Regression)\n",
    "어떠한 연속적인 값의 범위 내에서 예측값이 나오는 경우\n",
    "\n",
    "예를 들어서 역과의 거리, 인구 밀도, 방의 개수 등을 입력하면 부동산 가격을 예측하는 머신 러닝 모델이 있다고 해봅시다.\n",
    "머신 러닝 모델이 부동산 가격을 7억 8,456만 3,450원으로 예측하는 경우도 있을 것이고, 8억 1257만 300원으로 예측하는 경우도 있을 수 있습니다.\n",
    "특정 값의 범위 내에서는 어떤 숫자도 나올 수 있습니다.\n",
    "기존의 분류 문제와 같이 분리된(비연속적인) 답이 결과가 아니라 연속된 값을 결과로 가지는 이러한 문제를 회귀 문제라고 부릅니다.\n",
    "회귀 문제의 예시로 시계열 데이터(Time Series Data)를 이용한 주가 예측, 생산량 예측, 지수 예측 등이 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70056304dd9c7032"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. 지도 학습과 비지도 학습\n",
    "***\n",
    "머신 러닝은 크게 지도 학습, 비지도 학습, 강화 학습으로 나눕니다.\n",
    "그리고 큰 갈래로서는 자주 언급 되지는 않지만 딥 러닝 자연어 처리에서 중요한 학습 방법 중 하나인 자기지도 학습(Self-Supervised Learning, SSL)이 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78aaa5ded1b7f126"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1) 지도 학습(Supervised Learning)\n",
    "지도 학습이란 레이블(Label)이라는 정답과 함께 학습하는 것을 말합니다. 자연어 처리는 대부분 지도 학습에 속합니다.\n",
    "레이블이라는 말 외에도$y$, 실제값 등으로 부르기도 하는데 이 책에서는 이 용어들을 상황에 따라서 바꿔서 사용합니다.\n",
    "기계는 예측값과 실제값의 차이인 오차를 줄이는 방식으로 학습을 하게 되는데 예측값은 $\\hat{y}$과 같이 표현하기도 합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0688d4dbf4f2ec9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2) 비지도 학습(Unsupervised Learning)\n",
    "비지도 학습은 데이터에 별도의 레이블이 없이 학습하는 것을 말합니다.\n",
    "예를 들어 텍스트 처리 분야의 토픽 모델링 알고리즘인 LSA나 LDA는 비지도 학습에 속합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9ee63e4d0920eef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3) 자기지도 학습(Self-Supervised Learning, SSL)\n",
    "레이블이 없는 데이터가 주어지면, 모델이 학습을 위해서 스스로 데이터로부터 레이블을 만들어서 학습하는 경우를 자기지도 학습이라고 합니다.\n",
    "대표적인 예시로는 Word2Vec과 같은 워드 임베딩 알고리즘이나, BERT와 같은 언어 모델의 학습 방법을 들 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "502435ccb61c0b0b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. 샘플(Sample)과 특성(Feature)\n",
    "***\n",
    "많은 머신 러닝 문제가 1개 이상의 독립 변수 $x$를 가지고 종속 변수 $y$를 예측하는 문제입니다.\n",
    "머신 러닝 모델 중 특히 인공 신경망은 독립 변수, 종속 변수, 가중치, 편향 등을 행렬 연산을 통해 연산하는 경우가 많습니다.\n",
    "앞으로 인공 신경망을 배우게되면 훈련 데이터를 행렬로 표현하는 경우를 많이 보게 됩니다.\n",
    "독립 변수 $x$의 행렬을 X라고 하였을 때, 독립 변수의 개수가 n개이고 데이터의 개수가 m인 행렬 X는 다음과 같습니다.\n",
    "\n",
    "![행렬](img_2.png)\n",
    "\n",
    "\n",
    "이때 머신 러닝에서는 하나의 데이터. 행렬 관점에서는 하나의 행을 샘플(Sample)이라고 부릅니다.(데이터베이스에서 레코드라고 부르는 단위입니다.)\n",
    "그리고 종속 변수 $y$를 예측하기 위한 각각의 독립 변수 $x$를 특성(Feature)이라고 부릅니다. 행렬 관점에서는 각 열에 해당됩니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7b10d0e5c8d256f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. 혼동 행렬(Confusion Matrix)\n",
    "***\n",
    "머신 러닝에서는 맞춘 문제수를 전체 문제수로 나눈 값을 정확도(Accuracy)라고 합니다.\n",
    "하지만 정확도는 맞춘 결과와 틀린 결과에 대한 세부적인 내용을 알려주지는 않습니다.\n",
    "이를 위해서 사용하는 것이 혼동 행렬(Confusion Matrix)입니다.\n",
    "예를 들어 참(True)와 거짓(False) 둘 중 하나를 예측하는 문제였다고 가정해봅시다.\n",
    "아래의 혼동 행렬에서 각 열은 예측값을 나타내며, 각 행은 실제값을 나타냅니다.\n",
    "\n",
    "![표](img_3.png)\n",
    "\n",
    "머신 러닝에서는 다음과 같은 네 가지 케이스에 대해서 각각 TP, FP, FN, TN을 정의합니다.\n",
    "\n",
    "- True Positive(TP) : 실제 True인 정답을 True라고 예측 (정답)\n",
    "- False Positive(FP) : 실제 False인 정답을 True라고 예측 (오답)\n",
    "- False Negative(FN) : 실제 True인 정답을 False라고 예측 (오답)\n",
    "- True Negative(TN) : 실제 False인 정답을 False라고 예측 (정답)\n",
    "\n",
    "이 개념을 사용하면 정밀도(Precision)과 재현율(Recall)이 됩니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdf1eba808f2bafb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1) 정밀도(Precision)\n",
    "정밀도란 모델이 True라고 분류한 것 중에서 실제 True인 것의 비율입니다.\n",
    "\n",
    "$정밀도 =$$ TP \\over TP+FP $"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faedd6c4ea530672"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2) 재현율(Recall)\n",
    "재현율이란 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율입니다.\n",
    "\n",
    "$정밀도 =$$ TP \\over TP+FN $\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98166c84d0246141"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Precision이나 Recall은 모두 실제 True인 정답을 모델이 True라고 예측한 경우. 즉, TP에 관심이 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d06ef6bbb2ff51c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3) 정확도(Accuracy)\n",
    "\n",
    "정확도(Accuracy)는 우리가 일반적으로 실생활에서도 가장 많이 사용하는 지표입니다.\n",
    "전체 예측한 데이터 중에서 정답을 맞춘 것에 대한 비율입니다. TP, FP, FN, TN을 가지고 수식을 설명하면 다음과 같습니다.\n",
    "\n",
    "$정밀도 =$$ TP+TN \\over TP+FN+FP+TN $\n",
    " \n",
    "\n",
    "**Accuracy로 성능을 예측하는 것이 적절하지 않은 때**\n",
    "비가 오는 날을 예측하는 모델을 만들었다고 했을 때, 200일 동안 총 6일만 비가 왔다고 해봅시다.\n",
    "그런데 이 모델은 200일 내내 날씨가 맑았다고 예측했습니다. 이 모델은 200번 중 총 6회 틀렸습니다.\n",
    "194/200=0.97이므로 정확도는 97%입니다. 하지만 정작 비가 온 날은 하나도 못 맞춘 셈입니다.\n",
    "\n",
    "이렇게 실질적으로 더 중요한 경우에 대한 데이터가 전체 데이터에서 너무 적은 비율을 차지한다면 정확도는 좋은 측정 지표가 될 수 없습니다.\n",
    "이런 경우에는 F1-Score를 사용하며, 이에 대해서는 개체명 인식 챕터에서 설명하겠습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "899eb5a20e6081d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. 과적합(Overfitting)과 과소 적합(Underfitting)\n",
    "***\n",
    "\n",
    "**과적합(Overfitting)** : 훈련 데이터를 과하게 학습한 경우\n",
    "\n",
    "머신 러닝 모델이 학습에 사용하는 훈련 데이터는 실제로 앞으로 기계가 풀어야 할 현실의 수많은 문제에 비하면 극히 일부에 불과한 데이터입니다.\n",
    "기계가 훈련 데이터에 대해서만 과하게 학습하면 성능 측정을 위한 데이터인 테스트 데이터나 실제 서비스에서는 정확도가 좋지 않은 현상이 발생합니다.\n",
    "\n",
    "과적합 상황에서는 훈련 데이터에 대해서는 오차가 낮지만, 테스트 데이터에 대해서는 오차가 커집니다.\n",
    "아래의 그래프는 과적합 상황에서 발생할 수 있는 훈련 데이터에 대한 훈련 횟수에 따른 훈련 데이터의 오차와 테스트 데이터의 오차(또는 손실이라고도 부릅니다.)의 변화를 보여줍니다.\n",
    "\n",
    "\n",
    "![그래프](img_4.png)\n",
    "\n",
    "\n",
    "위 그래프는 뒤의 RNN을 이용한 텍스트 분류 챕터의 스팸 메일 분류하기 실습에서 훈련 데이터에 대한 훈련 횟수를 30 에포크로 주어서 의도적으로 과적합을 발생시킨 그래프입니다.\n",
    "y축은 오차(loss), X축의 에포크(epoch)는 전체 훈련 데이터에 대한 훈련 횟수를 의미하며, 사람으로 비유하면 동일한 문제지(훈련 데이터)를 반복해서 푼 횟수입니다.\n",
    "에포크가 지나치게 크면 훈련 데이터에 과적합이 발생합니다.\n",
    "\n",
    "스팸 메일 분류하기 실습은 에포크가 3~4에서 테스트 데이터에 대한 정확도가 가장 높고, 에포크가 그 이상을 넘어가면 과적합이 발생합니다.\n",
    "위의 그래프는 에포크가 증가할수록 테스트 데이터에 대한 오차가 점차 증가하는 양상을 보여줍니다.\n",
    "과적합은 다르게 설명하면 훈련 데이터에 대한 정확도는 높지만, 테스트 데이터는 정확도가 낮은 상황이라고 말할 수도 있습니다.\n",
    "이런 상황을 방지하기 위해서는 테스트 데이터의 오차가 증가하기 전이나, 정확도가 감소하기 전에 훈련을 멈추는 것이 바람직합니다.\n",
    "\n",
    "\n",
    "**과소적합(Underfitting)** : 테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 상태\n",
    "\n",
    "\n",
    "과소 적합은 훈련 자체가 부족한 상태이므로 훈련 횟수인 에포크가 지나치게 적으면 발생할 수 있습니다.\n",
    "과대 적합과는 달리 과소 적합은 훈련 자체를 너무 적게한 상태이므로 훈련 데이터에 대해서도 정확도가 낮다는 특징이 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "이러한 두 가지 현상을 과적합과 과소 적합이라고 부르는 이유는 머신 러닝에서 학습 또는 훈련이라고 하는 과정을 적합(fitting)이라고도 부르기 때문입니다.\n",
    "모델이 주어진 데이터에 대해서 적합해져가는 과정이기 때문입니다. 이러한 이유로 케라스에서는 기계를 학습시킬 때 fit()을 호출합니다.\n",
    "\n",
    "딥 러닝을 할 때는 과적합을 막을 수 있는 드롭 아웃(Dropout), 조기 종료(Early Stopping)과 같은 몇 가지 방법이 존재합니다.\n",
    "\n",
    "과적합과 과소 적합을 설명하면서 테스트 데이터를 사용하여 판단할 수 있다고 설명하였지만, 더 정확히 설명하면 현업에서는 테스트 데이터를 두 가지 용도로 분리하여 사용하는 것이 더 바람직합니다.\n",
    "- 과적합 모니터링과 하이퍼파라미터 튜닝을 위한 테스트 데이터 : **검증 데이터**\n",
    "- 오직 성능 평가만을 위한 테스트 데이터\n",
    "\n",
    "**과적합 방지를 고려한 일반적인 딥 러닝 모델의 학습 과정**\n",
    "\n",
    "Step 1. 주어진 데이터를 훈련 데이터, 검증 데이터, 테스트 데이터로 나눈다. 가령, 6:2:2 비율로 나눌 수 있다.\n",
    "Step 2. 훈련 데이터로 모델을 학습한다. (에포크 +1)\n",
    "Step 3. 검증 데이터로 모델을 평가하여 검증 데이터에 대한 정확도와 오차(loss)를 계산한다.\n",
    "Step 4. 검증 데이터의 오차가 증가하였다면 과적합 징후이므로 학습 종료 후 Step 5로 이동, 아니라면 Step 2.로 재이동한다.\n",
    "Step 5. 모델의 학습이 종료되었으니 테스트 데이터로 모델을 평가한다.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fde9963648d20e62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 06-03 선형 회귀(Linear Regression)\n",
    "***\n",
    "\n",
    "딥 러닝을 이해하기 위해서는 선형 회귀(Linear Regression)와 로지스틱 회귀(Logsitic Regression)를 이해할 필요가 있습니다.\n",
    "이번 챕터에서는 머신 러닝에서 쓰이는 용어인 가설(Hypothesis), 손실 함수(Loss Function) 그리고 경사 하강법(Gradient Descent)에 대한 개념과 선형 회귀에 대해서 이해합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c95e7bf0a10cca99"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 선형 회귀(Linear Regression)\n",
    "***\n",
    "다른 변수의 값을 변하게하는 변수를 $x$, 변수 $x$에 의해서 값이 종속적으로 변하는 변수 $y$라고 해봅시다.\n",
    "이때 변수 $x$의 값은 독립적으로 변할 수 있는 것에 반해, $y$값은 계속해서 $x$의 값에 의해서, 종속적으로 결정되므로 $x$를 독립 변수, $y$를 종속 변수라고도 합니다.\n",
    "선형 회귀는 한 개 이상의 독립 변수 $x$와 $y$의 선형 관계를 모델링합니다. 만약, 독립 변수 $x$가 1개라면 단순 선형 회귀라고 합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebd94166287f8f4f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1) 단순 선형 회귀 분석(Simple Linear Regression Analysis)\n",
    "- $y = wx+b$ <br/>\n",
    "위의 수식은 단순 선형 회귀의 수식을 보여줍니다.\n",
    "여기서 독립 변수 $x$와 곱해지는 값 $w$를 머신 러닝에서는 가중치(weight), 별도로 더해지는 값 $b$를 편향(bias)이라고 합니다.\n",
    "직선의 방정식에서는 각각 직선의 기울기와 절편을 의미합니다.\n",
    "$w$와 $b$가 없이 $y$와 $x$란 수식은 $y$는 $x$와 같다는 하나의 식밖에 표현하지 못합니다.\n",
    "그래프 상으로 말하면 하나의 직선밖에 표현하지 못합니다. <br/>\n",
    "- $y = x$ <br/>\n",
    "다시 말해 $w$와 $b$의 값에 따라서 $x$와 $y$가 표현하는 직선은 무궁무진해집니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a21dbe481ae8c2ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2) 다중 선형 회귀 분석(Multiple Linear Regression Analysis)\n",
    "![수식](img_5.png)\n",
    "집의 매매 가격은 단순히 집의 평수가 크다고 결정되는 게 아니라 집의 층의 수, 방의 개수, 지하철 역과의 거리와도 영향이 있습니다.\n",
    "이러한 다수의 요소를 가지고 집의 매매 가격을 예측해보고 싶습니다. \n",
    "$y$는 여전히 1개이지만 이제 $x$는 1개가 아니라 여러 개가 되었습니다.\n",
    "이를 다중 선형 회귀 분석이라고 합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91c1f1c0de7b911a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 가설(Hypothesis) 세우기\n",
    "***\n",
    "단순 선형 회귀를 가지고 문제를 풀어봅시다. 어떤 학생의 공부 시간에 따라서 다음과 같은 점수를 얻었다는 데이터가 있습니다.\n",
    "![데이터](img_6.png)\n",
    "이를 좌표 평면에 그려보면 다음과 같습니다.\n",
    "![데이터](img_7.png)\n",
    "알고있는 데이터로부터 $x$와 $y$의 관계를 유추하고, 이 학생이 6시간, 7시간, 8시간을 공부하였을 때의 성적을 예측해보고 싶습니다. \n",
    "$x$와 $y$의 관계를 유추하기 위해서 수학적으로 식을 세워보게 되는데 머신 러닝에서는 이러한 식을 **가설(Hypothesis)** 이라고 합니다.\n",
    "아래의 $H(x)$에서 $H$는 Hypothesis를 의미합니다.\n",
    "\n",
    "![데이터](img_8.png)\n",
    "\n",
    "위의 그림은 $w$와 $b$의 값에 따라서 천차만별로 그려지는 직선의 모습을 보여줍니다.\n",
    "중학교 수학 과정인 직선의 방정식을 알고있다면, 위의 가설에서 $w$는 직선의 기울기이고 $b$는 절편으로 직선을 표현함을 알 수 있습니다.\n",
    "결국 선형 회귀는 주어진 데이터로부터 $y$와 $x$의 관계를 가장 잘 나타내는 직선을 그리는 일을 말합니다.\n",
    "그리고 어떤 직선인지 결정하는 것은 $w$와 $b$의 값이므로 선형 회귀에서 해야할 일은 결국 적절한 $w$와 $b$를 찾아내는 일이 됩니다.\n",
    "\n",
    "\n",
    "아직은 방법을 모르지만, 어떤 방법을 사용하여 적절한 $w$와 $b$의 값을 찾은 덕택에 \n",
    "$y$와 $x$의 관계를 가장 잘 나타내는 직선을 위의 좌표 평면 상에서 그렸다고 한 번 가정해보겠습니다.\n",
    "이 직선을 $x$가 6일때, 7일때, 8일때에 대해서도 계속해서 직선을 그저 이어그린다면 이 학생이 6시간을 공부했을 때, 7시간을 공부했을 때, 8시간을 공부했을 때의 예상 점수를 말할 수 있게 됩니다.\n",
    "그저 $x$가 각각 6일 때, 7일 때, 8일 때의 $y$값을 확인하면 되기 때문입니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2531d0e7ee9317a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. 비용 함수(Cost function) : 평균 제곱 오차(MSE)\n",
    "***\n",
    "\n",
    "- 문제에 대한 규칙을 가장 잘 표현하는 $w$와 $b$를 찾기\n",
    "머신 러닝은 $w$와 $b$를 찾기 위해서 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세우고, 이 식의 값을 최소화하는 최적의 $w$와 $b$를 찾아낸다.\n",
    "\n",
    "이때 실제값과 예측값에 대한 오차에 대한 식 : **목적 함수(Objective function) / 비용 함수(Cost function) / 손실 함수(Loss function)**\n",
    "\n",
    "- **목적 함수(Objective function)** : 함수의 값을 최소화하거나, 최대화하거나 하는 목적을 가진 함수\n",
    "- **비용 함수(Cost function) or 손실 함수(Loss function)** : 값을 최소화하는 함수\n",
    "\n",
    "- **비용 함수(Cost function)** : 단순히 실제값과 예측값에 대한 오차를 표현 +  예측값의 오차를 줄이는 일에 최적화 된 식\n",
    "- 회귀 문제의 경우에는 주로 **평균 제곱 오차(Mean Squared Error, MSE)** 가 사용된다.\n",
    "\n",
    "\n",
    "![그래프](img_9.png)\n",
    "\n",
    "임의의 $w$의 값 13과 임의의 $b$의 값 1을 가진 직선\n",
    "이 직선으로부터 서서히 $w$와 $b$의 값을 바꾸면서 정답인 직선을 찾아내기\n",
    "\n",
    "$x$와 $y$ 의 관계를 가장 잘 나타내는 직선 = 위의 그림에서 모든 점들과 위치적으로 가장 가까운 직선\n",
    "\n",
    "**오차(error)의 정의**\n",
    "오차는 주어진 데이터에서 각 $x$에서의 실제값 $y$와 위의 직선에서 예측하고 있는 $H(x)$값의 차이를 말한다.\n",
    "위의 그림에서 ↕는 각 점에서의 오차의 크기를 보여준다.\n",
    "오차를 줄여가면서 $w$와 $b$의 값을 찾아내기 위해서는 전체 오차의 크기를 구해야 한다.\n",
    "\n",
    "오차의 크기를 측정하기 위한 가장 기본적인 방법 : 각 오차를 모두 더함\n",
    "\n",
    "\n",
    "위의 $y = 13x + 1$ 직선이 예측한 예측값을 각각 실제값으로부터 오차를 계산한 표\n",
    "\n",
    "![표](img_10.png)\n",
    "\n",
    "\n",
    "수식적으로 단순히 '오차 = 실제값 - 예측값' 이라고 정의한 후에 모든 오차를 더하면 음수 오차도 있고, 양수 오차도 있으므로 오차의 절대적인 크기를 구할 수가 없다.\n",
    "그래서 모든 오차를 제곱하여 더하는 방법을 사용(절댓값 사용)\n",
    "이를 수식으로 표현하면 아래와 같다. (단, 여기서 $n$은 갖고 있는 데이터의 개수를 의미)\n",
    "(실제값-예측값)^ 한 값 모두 더하기(시그마)\n",
    "\n",
    "![표](img_11.png)\n",
    "\n",
    "이때 데이터의 개수인 $n$으로 나누면, 오차의 제곱합에 대한 평균을 구할 수 있다.\n",
    "이를 **평균 제곱 오차(Mean Squered Error, MSE)** 라고 한다.\n",
    "\n",
    "![표](img_12.png)\n",
    "\n",
    "$y = 13x + 1$의 예측값과 실제값의 평균 제곱 오차의 값은 52.5이다.\n",
    "평균 제곱 오차의 값을 최소값으로 만드는 $w$와 $b$를 찾아내는 것이 정답인 직선을 찾아내는 일이다.\n",
    "평균 제곱 오차를 $w$와 $b$에 의한 비용 **함수(Cost function)** 로 재정의해보면 다음과 같다.\n",
    "\n",
    "![표](img_13.png)\n",
    "\n",
    "모든 점들과의 오차가 클 수록 평균 제곱 오차는 커지며, 오차가 작아질 수록 평균 제곱 오차는 작아진다.\n",
    "그러므로 이 평균 최소 오차. 즉, $Cost(w, b)$를 최소가 되게 만드는 $w$와 $b$를 구하면 결과적으로 $y$와 $x$의 관계를 가장 잘 나타내는 직선을 그릴 수 있다.\n",
    "\n",
    "![표](img_14.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1289fe99f6819049"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. 옵티마이저(Optimizer) : 경사하강법(Gradient Descent)\n",
    "***\n",
    "\n",
    "선형 회귀를 포함한 수많은 머신 러닝, 딥 러닝의 학습은 결국 비용 함수를 최소화하는 매개 변수인 $w$와 $b$을 찾기 위한 작업을 수행합니다.\n",
    "이때 사용되는 알고리즘을 옵티마이저(Optimizer) 또는 최적화 알고리즘이라고 부릅니다.\n",
    "\n",
    "그리고 이 옵티마이저를 통해 적절한 $w$와 $b$를 찾아내는 과정을 머신 러닝에서 훈련(training) 또는 학습(learning)이라고 부릅니다.\n",
    "여기서는 가장 기본적인 옵티마이저 알고리즘인 **경사 하강법(Gradient Descent)** 에 대해서 배웁니다.\n",
    "\n",
    "경사 하강법을 이해하기 위해서 cost와 기울기 $w$와의 관계를 이해해보겠습니다.\n",
    "$w$는 머신 러닝 용어로는 가중치라고 불리지만, 직선의 방정식 관점에서 보면 직선의 기울기를 의미하고 있습니다.\n",
    "아래의 그래프는 기울기 $w$가 지나치게 높거나, 낮을 때 어떻게 오차가 커지는 모습을 보여줍니다.\n",
    "\n",
    "![그래프](img_15.png)\n",
    "\n",
    "각각 $y = 20x, y = x$에 해당되는 직선\n",
    "- 주황색선 :  기울기 $w$가 20\n",
    "- 초록색선 : 기울기 $w$가 1\n",
    "\n",
    "↕는 각 점에서의 실제값과 두 직선의 예측값과의 오차를 보여준다. 이는 $y = 13x + 1$ 직선보다 확연히 큰 오차값이다.\n",
    "즉, 기울기가 지나치게 크면 실제값과 예측값의 오차가 커지고, 기울기가 지나치게 작아도 실제값과 예측값의 오차가 커진다.\n",
    "사실 $b$또한 마찬가지인데 $b$가 지나치게 크거나 작으면 오차가 커진다.\n",
    "\n",
    "$w$와 cost의 관계를 그래프로 표현하면 다음과 같다.\n",
    "- 설명의 편의를 위해 편향 $b$가 없이 단순히 가중치 $w$만을 사용한 $y = wx$라는 가설 $H(x)$를 가지고, 경사 하강법을 수행\n",
    "- 비용 함수의 값 $cost(x)$는 cost라고 줄여서 표현\n",
    "\n",
    "![그래프](img_16.png)\n",
    "\n",
    "- 기울기 $w$가 무한대로 커짐 -> cost의 값 무한대로 커짐\n",
    "- 기울기 $w$가 무한대로 작아짐 -> cost의 값 무한대로 커짐\n",
    "\n",
    "위의 그래프에서 cost가 가장 작을 때는 볼록한 부분의 맨 아래 부분\n",
    "기계가 해야할 일은 cost가 가장 최소값을 가지게 하는 $w$를 찾는 일이므로, 볼록한 부분의 맨 아래 부분의 $w$의 값을 찾아야 한다.\n",
    "\n",
    "![그래프](img_17.png)\n",
    "\n",
    "기계는 임의의 랜덤값 $w$값을 정한 뒤에, 맨 아래의 볼록한 부분을 향해 점차 $w$의 값을 수정해나간다.\n",
    "위의 그림은 $w$값이 점차 수정되는 과정을 보여준다.\n",
    "그리고 이를 가능하게 하는 것이 **경사 하강법(Gradient Descent)** 이다.\n",
    "경사 하강법은 접선에서의 기울기의 개념을 사용한다.\n",
    "\n",
    "![그래프](img_18.png)\n",
    "\n",
    "위의 그림에서 초록색 선은 $w$가 임의의 값을 가지게 되는 네 가지의 경우에 대해서, 그래프 상으로 접선의 기울기를 보여준다.\n",
    "맨 아래의 볼록한 부분으로 갈수록 접선의 기울기가 점차 작아진다.\n",
    "맨 아래의 볼록한 부분에서는 접선의 기울기가 0. (그래프 상으로 초록색 화살표가 수평이 되는 지점)\n",
    "\n",
    "즉, cost가 최소화가 되는 지점은 접선의 기울기가 0이 되는 지점이며, 또한 미분값이 0이 되는 지점이다.\n",
    "경사 하강법의 아이디어는 비용 함수(Cost function)를 미분하여 현재 $w$에서의 접선의 기울기를 구하고, 접선의 기울기가 낮은 방향으로 \n",
    "$W$의 값을 변경하고 다시 미분하고 이 과정을 접선의 기울기가 0인 곳을 향해 $w$의 값을 변경하는 작업을 반복하는 것에 있습니다.\n",
    "\n",
    "비용(cost)를 최소화하는 $w$를 구하기 위해 $w$를 업데이트하는 식은 다음과 같다.\n",
    "이를 접선의 기울기가 0이 될 때까지 반복한다.\n",
    "![식](img_19.png)\n",
    "\n",
    "위의 식은 현재 $w$에서의 접선의 기울기와 $a$와 곱한 값을 현재 $w$에서 빼서 새로운 $w$의 값으로 한다는 것을 의미합니다. \n",
    "$a$는 여기서 학습률(learning rate)이라고 합니다.\n",
    "\n",
    "우선 $a$는 생각하지 않고 현재 $w$에서 현재 $w$에서의 접선의 기울기를 빼는 행위가 어떤 의미가 있는지 알아보겠습니다.\n",
    "\n",
    "![그래프](img_20.png)\n",
    "\n",
    "위의 그림은 접선의 기울기가 음수일 때, 0일때, 양수일 때를 보여줍니다. 접선의 기울기가 음수일 때의 수식은 아래와 같이 표현할 수 있습니다.\n",
    "$w := w-a(음수 기울기)$\n",
    "\n",
    "기울기가 음수면 '음수를 빼는 것'은 곧 '해당 값을 양수로 바꾸고 더하는 것'과 같습니다.\n",
    "(가령, 어떤 수에서 -2를 뺀다는 것은 해당 숫자에 2를 더하는 것과 같습니다.)\n",
    "결국 음수 기울기를 빼면 $w$의 값이 증가하게 되는데 이는 결과적으로 접선의 기울기가 0인 방향으로 $w$의 값이 조정됩니다.\n",
    "만약, 접선의 기울기가 양수라면 위의 수식은 아래와 같이 표현할 수 있습니다.\n",
    "$w := w-a(양수 기울기)$\n",
    "\n",
    "기울기가 양수면 $w$의 값이 감소하게 되는데 이는 결과적으로 기울기가 0인 방향으로 $w$의 값이 조정됩니다.\n",
    "결국, 아래의 수식은 접선의 기울기가 음수거나, 양수일 때 모두 접선의 기울기가 0인 방향으로 $w$의 값을 조정합니다.\n",
    "![식](img_19.png)\n",
    "\n",
    "\n",
    "그렇다면 여기서 학습률(learning rate)이라고 말하는 $a$는 어떤 의미를 가질까요?\n",
    "학습률 $a$은 $w$의 값을 변경할 때, 얼마나 크게 변경할지를 결정하며 0과 1사이의 값을 가지도록 합니다. 예를 들어서 0.01이 될 수 있겠습니다.\n",
    "학습률은 $w$를 그래프의 한 점으로보고 접선의 기울기가 0일 때까지 경사를 따라 내려간다는 관점에서는 얼마나 큰 폭으로 이동할지를 결정합니다.\n",
    "직관적으로 생각하기에 학습률 $a$의 값을 무작정 크게 하면 접선의 기울기가 최소값이 되는 $w$를 빠르게 찾을 수 있을 것 같지만 그렇지 않습니다.\n",
    "\n",
    "![그래프](img_21.png)\n",
    "\n",
    "위의 그림은 학습률 $a$가 지나치게 높은 값을 가질 때, 접선의 기울기가 0이 되는 $w$를 찾아가는 것이 아니라 $cost$의 값이 발산하는 상황을 보여줍니다.\n",
    "반대로 학습률 $a$가 지나치게 낮은 값을 가지면 학습 속도가 느려지므로 적당한 $a$의 값을 찾아내는 것도 중요합니다.\n",
    "지금까지는 $b$는 배제시키고 최적의 $w$를 찾아내는 것에만 초점을 맞추어 경사 하강법의 원리에 대해서 배웠는데,\n",
    "실제 경사 하강법은 $w$와 $b$에 대해서 동시에 경사 하강법을 수행하면서 최적의 \n",
    "$w$와 $b$의 값을 찾아갑니다.\n",
    "\n",
    "정리하자면 가설, 비용 함수, 옵티마이저는 머신 러닝 분야에서 사용되는 포괄적 개념입니다.\n",
    "풀고자하는 각 문제에 따라 가설, 비용 함수, 옵티마이저는 전부 다를 수 있으며 선형 회귀에 가장 적합한 비용 함수와 옵티마이저가 알려져있는데\n",
    "이번 챕터에서 언급된 MSE와 경사 하강법이 각각 이에 해당됩니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd96f7afad303657"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
