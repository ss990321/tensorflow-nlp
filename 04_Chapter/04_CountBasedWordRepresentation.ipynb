{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# 04. 카운트 기반의 단어 표현(Count based word Representation)\n",
    "***\n",
    "\n",
    "자연어 처리에서 텍스트를 표현하는 방법으로는 여러가지 방법이 있습니다. 이번 챕터에서는 그 중 정보 검색과 텍스트 마이닝 분야에서 주로 사용되는 카운트 기반의 텍스트 표현 방법인 DTM(Document Term Matrix)과 TF-IDF(Term Frequency-Inverse Document Frequency)에 대해서 다룹니다.\n",
    "\n",
    "텍스트를 위와 같은 방식으로 수치화를 하고나면, 통계적인 접근 방법을 통해 여러 문서로 이루어진 텍스트 데이터가 있을 때 어떤 단어가 특정 문서 내에서 얼마나 중요한 것인지를 나타내거나, 문서의 핵심어 추출, 검색 엔진에서 검색 결과의 순위 결정, 문서들 간의 유사도를 구하는 등의 용도로 사용할 수 있습니다.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9545a6d1eac195eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 04-01 다양한 단어의 표현 방법\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c976597abf082774"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 단어의 표현 방법\n",
    "***\n",
    "\n",
    "- **국소 표현(Local Representation) 방법**\n",
    "    + 국소 표현 방법은 해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법\n",
    "    + 이산 표현(Discrete Representation)이라고도 한다.\n",
    "- **분산 표현(Distributed Representation) 방법**\n",
    "    + 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법\n",
    "    + 연속 표현(Continuous Represnetation)이라고 표현하기도 한다.\n",
    "\n",
    "**이 두 방법의 차이** \n",
    "- 국소 표현 방법은 단어의 의미, 뉘앙스를 표현할 수 없다.\n",
    "- 분산 표현 방법은 단어의 뉘앙스를 표현할 수 있게 됩니다.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "951bc0dfc4000c5b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 단어 표현의 카테고리화\n",
    "***\n",
    "아래와 같은 기준으로 단어 표현을 카테고리화하여 작성되었습니다.\n",
    "![단어표현 카테고리화](img.png) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec054d50d62740f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이번 챕터의 **Bag of Words**는 **국소 표현**에(Local Representation)에 속하며, 단어의 빈도수를 카운트(Count)하여 단어를 수치화하는 단어 표현 방법입니다.\n",
    "이 챕터에서는 **BoW**와 그의 확장인 **DTM**(또는 TDM)에 대해서 학습하고, 이러한 빈도수 기반 단어 표현에 단어의 중요도에 따른 가중치를 줄 수 있는 **TF-IDF**에 대해서 학습합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13f622f6d311f109"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 04-02 Bag of Words(BoW)\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2836357dd04d650c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Bag of Words란?\n",
    "***\n",
    "Bag of Words란 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법입니다.\n",
    "단어의 순서는 중요하지 않습니다.\n",
    "\n",
    "BoW를 만드는 두 가지 과정\n",
    "**(1)** 각 단어에 고유한 정수 인덱스를 부여합니다.  # 단어 집합 생성.\n",
    "**(2)** 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다.  \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85793abb77b1486a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def build_bag_of_words(document):\n",
    "  # 온점 제거 및 형태소 분석\n",
    "  document = document.replace('.', '')\n",
    "  tokenized_document = okt.morphs(document)\n",
    "\n",
    "  word_to_index = {}\n",
    "  bow = []\n",
    "\n",
    "  for word in tokenized_document:  \n",
    "    if word not in word_to_index.keys(): # word_to_index에 단어가 없으면\n",
    "      word_to_index[word] = len(word_to_index)  \n",
    "      # BoW에 전부 기본값 1을 넣는다.\n",
    "      bow.insert(len(word_to_index) - 1, 1) # 인덱스 0부터\n",
    "      # 위에서 word_to_index에 값 넣어주었기 때문에 길이가 1 되어서 bow.insert 할 때는 인덱스 0부터 넣어주기 위해서 -1 해준다.\n",
    "    else:\n",
    "      # 재등장하는 단어의 인덱스\n",
    "      index = word_to_index.get(word)\n",
    "      # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더한다.\n",
    "      bow[index] = bow[index] + 1\n",
    "\n",
    "  return word_to_index, bow"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T05:30:18.759955100Z",
     "start_time": "2024-01-11T05:30:18.750498100Z"
    }
   },
   "id": "feaaf84759f8f635",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
      "bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\n",
    "vocab, bow = build_bag_of_words(doc1)\n",
    "print('vocabulary :', vocab) # word_to_index\n",
    "print('bag of words vector :', bow)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T05:30:21.649530300Z",
     "start_time": "2024-01-11T05:30:19.444397600Z"
    }
   },
   "id": "f56186567a11e23d",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Bag of Words의 다른 예제들\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "230c02b586c22f37"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary : {'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
      "bag of words vector : [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc2 = '소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.'\n",
    "\n",
    "vocab, bow = build_bag_of_words(doc2)\n",
    "print('vocabulary :', vocab)\n",
    "print('bag of words vector :', bow)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T05:30:22.589425600Z",
     "start_time": "2024-01-11T05:30:22.559905400Z"
    }
   },
   "id": "e6636bdeb84ecf02",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\n",
      "bag of words vector : [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# 문서1과 문서2를 합쳐서 문서 3이라고 명명하고, BoW를 만들 수도 있다.\n",
    "\n",
    "doc3 = doc1 + ' ' + doc2\n",
    "vocab, bow = build_bag_of_words(doc3)\n",
    "print('vocabulary :', vocab)\n",
    "print('bag of words vector :', bow)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T05:31:19.654232Z",
     "start_time": "2024-01-11T05:31:19.625605600Z"
    }
   },
   "id": "4b3ad899f7b1c588",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "BoW는 종종 여러 문서의 단어 집합을 합친 뒤에, 해당 단어 집합에 대한 각 문서의 BoW를 구하기도 한다. 가령, 문서3에 대한 단어 집합을 기준으로 문서1, 문서2의 BoW를 만든다고 한다면 결과는 아래와 같다.\n",
    "***\n",
    "#### 문서3 단어 집합에 대한 문서1 BoW : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
    "#### 문서3 단어 집합에 대한 문서2 BoW : [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1]  \n",
    "***\n",
    "문서3 단어 집합에서 물가상승률이라는 단어는 인덱스가 4에 해당된다. 물가상승률이라는 단어는 문서1에서는 2회 등장하며, 문서2에서는 1회 등장하였기 때문에 두 BoW의 인덱스 4의 값은 각각 2와 1이 되는 것을 볼 수 있다.\n",
    "\n",
    "BoW는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이므로 주로 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰인다.\n",
    "즉, 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 주로 쓰인다.\n",
    "가령, '달리기', '체력', '근력'과 같은 단어가 자주 등장하면 해당 문서를 체육 관련 문서로 분류할 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ec73ea583ae495c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. CountVectorizer 클래스로 BoW 만들기\n",
    "***\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfd7fd99acc07dbb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [1 1 2 1 2 1]\n",
      "vocabulary : [('because', 0), ('know', 1), ('love', 2), ('want', 3), ('you', 4), ('your', 5)]\n"
     ]
    }
   ],
   "source": [
    "# 사이킷 런에서는 단어의 빈도를 Count하여 Vector로 만드는 CountVectorizer 클래스를 지원한다.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer() # 기본적으로 길이가 2이상인 것만 인식(정제작업)\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print('bag of words vector :', vector.fit_transform(corpus).toarray()[0]) \n",
    "\n",
    "# 각 단어의 인덱스가 어떻게 부여되었는지를 출력\n",
    "print('vocabulary :',sorted(vector.vocabulary_.items()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T07:11:33.583660800Z",
     "start_time": "2024-01-16T07:11:33.574658300Z"
    }
   },
   "id": "2e8d6ad74f42d9c0",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "**주의할 점** \n",
    "CountVectorizer는 단지 띄어쓰기만을 기준으로 단어를 자르는 낮은 수준의 토큰화를 진행하고 BoW를 만든다.\n",
    "이는 한국어에 CountVectorizer를 적용하면, 조사 등의 이유로 제대로 BoW가 만들어지지 않음을 의미한다.\n",
    "\n",
    "EX) \n",
    "**'정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.'** 라는 문장을 CountVectorizer를 사용하여 BoW로 만들 경우\n",
    "- CountVectorizer는 '물가상승률'이라는 단어를 인식하지 못 한다.\n",
    "- CountVectorizer는 띄어쓰기를 기준으로 분리한 뒤에 '물가상승률과'와 '물가상승률은' 으로 조사를 포함해서 하나의 단어로 판단하기 때문에 서로 다른 두 단어로 인식한다.\n",
    "- 그리고 '물가상승률과'와 '물가상승률은'이 각자 다른 인덱스에서 1이라는 빈도의 값을 갖게 된다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4e6d53a7d5ee8a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. 불용어를 제거한 BoW 만들기\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a9851362c7dd287"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 영어의 BoW를 만들기 위해 사용하는 CountVectorizer는 불용어를 지정하면\n",
    "# 불용어는 제외하고 BoW를 만들 수 있도록 불용어 제거 기능을 지원한다.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T06:32:33.158492900Z",
     "start_time": "2024-01-11T06:32:32.523666300Z"
    }
   },
   "id": "7e3c8af748777128",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (1) 사용자가 직접 정의한 불용어 사용"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54879ab2c95e82d6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1 1 1]]\n",
      "vocabulary : {'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
    "print('vocabulary :',vect.vocabulary_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T06:33:19.064292700Z",
     "start_time": "2024-01-11T06:33:19.058281Z"
    }
   },
   "id": "bb26f8be1a0148a0",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (2) CountVectorizer에서 제공하는 자체 불용어 사용"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6531ec3d7e998710"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1]]\n",
      "vocabulary : {'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "# 영어의 BoW를 만들기 위해 사용하는 CountVectorizer는 불용어를 지정하면\n",
    "# 불용어는 제외하고 BoW를 만들 수 있도록 불용어 제거 기능을 지원한다.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=\"english\") # 'everything'을 불용어로 간주함\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
    "print('vocabulary :',vect.vocabulary_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T06:33:45.922320300Z",
     "start_time": "2024-01-11T06:33:45.918256400Z"
    }
   },
   "id": "83323d0e6b4dedfd",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (3) NLTK에서 지원하는 불용어 사용"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a80651cbf6564e79"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1 1]]\n",
      "vocabulary : {'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "stop_words = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words=stop_words)\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray()) \n",
    "print('vocabulary :',vect.vocabulary_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T06:34:06.042722200Z",
     "start_time": "2024-01-11T06:34:06.029876700Z"
    }
   },
   "id": "4a5172798b90426c",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 04-03 문서 단어 행렬(Document-Term Matrix, DTM)\n",
    "***\n",
    "\n",
    "**문서 단어 행렬(Document-Term Matrix, DTM)**\n",
    "- 서로 다른 문서들의 BoW들을 결합한 표현 방법 (이하 DTM이라고 명명)\n",
    "- 행과 열을 반대로 선택하면 TDM이라고 부르기도 합니다.\n",
    "- 서로 다른 문서들을 비교할 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97fc87700f98ea88"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 문서 단어 행렬(Document-Term Matrix, DTM)의 표기법\n",
    "***\n",
    "\n",
    "정의 : 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것\n",
    "\n",
    "- 각 문서에 대한 BoW를 하나의 행렬로 만든 것으로 생각할 수 있으며, \n",
    "- BoW와 다른 표현 방법이 아니라 BoW 표현을 다수의 문서에 대해서 행렬로 표현하고 부르는 용어"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bc32fa5cca96a31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| 문서 | 내용           |\n",
    "|---|--------------|\n",
    "| 문서1 | 먹고 싶은 사과     |\n",
    "| 문서2 | 먹고 싶은 바나나    |\n",
    "| 문서3 | 길고 노란 바나나 바나나 |\n",
    "| 문서4 | 저는 과일이 좋아요   |\n",
    "\n",
    "**문서 단어 행렬로 표현**\n",
    "\n",
    "\n",
    "![문서단어행렬](img_1.png)\n",
    "\n",
    "\n",
    "각 문서에서 등장한 단어의 빈도를 행렬의 값으로 표기\n",
    "문서 단어 행렬은 문서들을 서로 비교할 수 있도록 수치화할 수 있다는 점에서 의의를 갖는다.\n",
    "만약 필요에 따라서는 형태소 분석기로 단어 토큰화를 수행하고, 불용어에 해당되는 조사들 또한 제거하여 더 정제된 DTM을 만들 수도 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2bf3204699d8b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 문서 단어 행렬(Document-Term Matrix)의 한계\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b613c628b55a8c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1) 희소 표현(Sparse representation)\n",
    "\n",
    "- 공간적 낭비와 계산 리소스를 증가시킬 수 있다.\n",
    "- 만약 가지고 있는 전체 코퍼스가 방대한 데이터라면 문서 벡터의 차원은 수만 이상의 차원을 가질 수도 있다.\n",
    "- 또한 많은 문서 벡터가 대부분의 값이 0을 가질 수도 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ad4766dce10f4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2) 단순 빈도 수 기반 접근\n",
    "\n",
    "- 영어에 대해서 DTM을 만들었을 때, 불용어인 the는 어떤 문서이든 자주 등장할 수 밖에 없다.\n",
    "- 유사한 문서인지 비교하고 싶은 문서1, 문서2, 문서3에서 동일하게 the가 빈도수가 높다고 해서 이 문서들이 유사한 문서라고 판단해서는 안 된다.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15abda7a2d7ee12b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 04-04 TF-IDF(Term Frequency-Inverse Document Frequency)\n",
    "***\n",
    "\n",
    "TF-IDF 가중치\n",
    "- DTM 내에 있는 각 단어에 대한 중요도를 계산\n",
    "- 기존의 DTM을 사용하는 것보다 보다 많은 정보를 고려하여 문서들을 비교할 수 있음"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f02ccf01b52a307"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. TF-IDF(단어 빈도-역 문서 빈도, Term Frequency-Inverse Document Frequency)\n",
    "***\n",
    "\n",
    "**TF-IDF(Term Frequency-Inverse Document Frequency)**\n",
    "- 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법입니다.\n",
    "- 우선 DTM을 만든 후, TF-IDF 가중치를 부여합니다.\n",
    "- 주로 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 쓰일 수 있습니다.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc32abe1c148a8ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TF-IDF 식 이해하기\n",
    "\n",
    "**TF-IDF는 TF와 IDF를 곱한 값을 의미**\n",
    "\n",
    "문서를 d, 단어를 t, 문서의 총 개수를 n이라고 표현할 때 TF, DF, IDF는 각각 다음과 같이 정의할 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c324f32b4d68c08"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (1) tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수.\n",
    "DTM의 예제에서 각 단어들이 가진 값\n",
    "각 문서에서의 각 단어의 등장 빈도를 나타내는 값"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "475438f3369541"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (2) df(t) : 특정 단어 t가 등장한 문서의 수.\n",
    "\n",
    "오직 특정 단어 t가 등장한 문서의 수에만 관심을 가진다. \n",
    "앞서 배운 DTM에서 바나나는 문서2와 문서3에서 등장했기 때문에 이 경우, 바나나의 df는 2이다.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c00a7353d9794369"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (3) idf(t) : df(t)에 반비례하는 수.\n",
    "\n",
    "\n",
    "![idf(t) 수식](img_2.png)\n",
    "\n",
    " 불용어 등과 같이 자주 쓰이는 단어들은 비교적 자주 쓰이지 않는 단어들보다 최소 수십 배 자주 등장한다.\n",
    " 그런데 비교적 자주 쓰이지 않는 단어들조차 희귀 단어들과 비교하면 또 최소 수백 배는 더 자주 등장하는 편이다.\n",
    " 이 때문에 log를 씌워주지 않으면, 희귀 단어들에 엄청난 가중치가 부여될 수 있다.\n",
    " 로그를 씌우면 이런 격차를 줄이는 효과가 있다.\n",
    " \n",
    "\n",
    " **log 안의 식에서 분모에 1을 더해주는 이유 : 특정 단어가 전체 문서에서 등장하지 않을 경우에 분모가 0이 되는 상황을 방지하기 위해서**\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43b91919245fa276"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단한다.\n",
    "\n",
    "- TF-IDF 값이 낮으면 중요도가 낮음\n",
    "- TF-IDF 값이 크면 중요도가 큼 \n",
    "\n",
    "즉, the나 a와 같이 불용어의 경우에는 모든 문서에 자주 등장하기 마련이기 때문에 자연스럽게 불용어의 TF-IDF의 값은 다른 단어의 TF-IDF에 비해서 낮아지게 된다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9da1c7634fe8d552"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TF-IDF 이해하기\n",
    "\n",
    "\n",
    "![문서단어행렬](img_1.png)\n",
    "\n",
    "\n",
    "우선 TF는 앞서 사용한 DTM을 그대로 사용하면, 그것이 각 문서에서의 각 단어의 TF가 됩니다.\n",
    "이제 구해야할 것은 TF와 곱해야할 값인 IDF입니다.\n",
    "IDF 계산을 위해 사용하는 로그의 밑은 TF-IDF를 사용하는 사용자가 임의로 정할 수 있는데, 여기서 로그는 마치 기존의 값에 곱하여 값의 크기를 조절하는 상수의 역할을 합니다.\n",
    "각종 프로그래밍 언어에서 패키지로 지원하는 TF-IDF의 로그는 대부분 자연 로그를 사용합니다. (자연 로그 -> ln)\n",
    "\n",
    "![IDF표](img_3.png)\n",
    "\n",
    "\n",
    "문서의 총 수는 4이기 때문에 ln 안에서 분자는 늘 4으로 동일합니다.\n",
    "분모의 경우에는 각 단어가 등장한 문서의 수(DF)를 의미하는데, 예를 들어서 '먹고'의 경우에는 총 2개의 문서(문서1, 문서2)에 등장했기 때문에 2라는 값을 가집니다.\n",
    "각 단어에 대해서 IDF의 값을 비교해보면 문서 1개에만 등장한 단어와 문서2개에만 등장한 단어는 값의 차이를 보입니다.\n",
    "IDF는 여러 문서에서 등장한 단어의 가중치를 낮추는 역할을 하기 때문입니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa4b01535c6f70b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TF-IDF 계산결과**\n",
    "각 단어의 TF는 DTM에서의 각 단어의 값과 같으므로, 앞서 사용한 DTM에서 단어 별로 위의 IDF값을 곱해주면 TF-IDF 값을 얻습니다.\n",
    "\n",
    "![TF-IDF 계산결과](img_4.png)\n",
    "\n",
    "\n",
    "문서3에서의 바나나만 TF 값이 2이므로 IDF에 2를 곱해주고, 나머진 TF 값이 1이므로 그대로 IDF 값을 가져오면 됩니다.\n",
    "문서2에서의 바나나의 TF-IDF 가중치와 문서3에서의 바나나의 TF-IDF 가중치가 다른 것을 볼 수 있습니다.(TF가 각각 1과 2로 다름)\n",
    "TF-IDF에서의 관점에서 보자면 TF-IDF는 특정 문서에서 자주 등장하는 단어는 그 문서 내에서 중요한 단어로 판단하기 때문입니다.\n",
    "**문서2에서는 바나나를 한 번 언급했지만, 문서3에서는 바나나를 두 번 언급했기 때문에 문서3에서의 바나나를 더욱 중요한 단어라고 판단하는 것입니다.**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ee063c48b3747be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 파이썬으로 TF-IDF 직접 구현하기\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33d29ca7a5d0c5a5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['과일이', '길고', '노란', '먹고', '바나나', '사과', '싶은', '저는', '좋아요']\n"
     ]
    }
   ],
   "source": [
    "# 앞의 설명에서 사용한 4개의 문서를 docs에 저장\n",
    "\n",
    "import pandas as pd # 데이터프레임 사용을 위해\n",
    "from math import log # IDF 계산을 위해\n",
    "\n",
    "docs = [\n",
    "  '먹고 싶은 사과',\n",
    "  '먹고 싶은 바나나',\n",
    "  '길고 노란 바나나 바나나',\n",
    "  '저는 과일이 좋아요'\n",
    "] \n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()\n",
    "print(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T07:48:36.133012200Z",
     "start_time": "2024-01-17T07:48:35.045523500Z"
    }
   },
   "id": "cd096384c04f322e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TF, IDF, 그리고 TF-IDF 값을 구하는 함수를 구현\n",
    "\n",
    "# 총 문서의 수\n",
    "N = len(docs) \n",
    "\n",
    "def tf(t, d):\n",
    "  return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "  df = 0\n",
    "  for doc in docs:\n",
    "    df += t in doc\n",
    "  return log(N/(df+1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "  return tf(t,d)* idf(t)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T07:48:36.889930900Z",
     "start_time": "2024-01-17T07:48:36.877963400Z"
    }
   },
   "id": "833e9f341afe433c",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n0    0   0   0   1    0   1   1   0    0\n1    0   0   0   1    1   0   1   0    0\n2    0   1   1   0    2   0   0   0    0\n3    1   0   0   0    0   0   0   1    1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>과일이</th>\n      <th>길고</th>\n      <th>노란</th>\n      <th>먹고</th>\n      <th>바나나</th>\n      <th>사과</th>\n      <th>싶은</th>\n      <th>저는</th>\n      <th>좋아요</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF를 구하기 : DTM을 데이터프레임에 저장하여 출력\n",
    "\n",
    "result = []\n",
    "\n",
    "# 각 문서에 대해서 아래 연산을 반복\n",
    "for i in range(N):\n",
    "  result.append([]) # 리스트 안에 리스트\n",
    "  d = docs[i]\n",
    "  for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result[-1].append(tf(t, d)) # 위에서 어펜드 해준 []에다가 넣기\n",
    "\n",
    "tf_ = pd.DataFrame(result, columns = vocab)\n",
    "tf_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T07:49:22.100952Z",
     "start_time": "2024-01-17T07:49:22.085880300Z"
    }
   },
   "id": "8dd221b3060060f4",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "          IDF\n과일이  0.693147\n길고   0.693147\n노란   0.693147\n먹고   0.287682\n바나나  0.287682\n사과   0.693147\n싶은   0.287682\n저는   0.693147\n좋아요  0.693147",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IDF</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>과일이</th>\n      <td>0.693147</td>\n    </tr>\n    <tr>\n      <th>길고</th>\n      <td>0.693147</td>\n    </tr>\n    <tr>\n      <th>노란</th>\n      <td>0.693147</td>\n    </tr>\n    <tr>\n      <th>먹고</th>\n      <td>0.287682</td>\n    </tr>\n    <tr>\n      <th>바나나</th>\n      <td>0.287682</td>\n    </tr>\n    <tr>\n      <th>사과</th>\n      <td>0.693147</td>\n    </tr>\n    <tr>\n      <th>싶은</th>\n      <td>0.287682</td>\n    </tr>\n    <tr>\n      <th>저는</th>\n      <td>0.693147</td>\n    </tr>\n    <tr>\n      <th>좋아요</th>\n      <td>0.693147</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어에 대한 IDF 값을 구하기\n",
    "\n",
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index=vocab, columns=[\"IDF\"])\n",
    "idf_ # 위에서 수기로 구한 IDF 값들과 정확히 일치"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T07:48:47.254462200Z",
     "start_time": "2024-01-17T07:48:47.240935400Z"
    }
   },
   "id": "b848bfa2cb475181",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "        과일이        길고        노란        먹고       바나나        사과        싶은  \\\n0  0.000000  0.000000  0.000000  0.287682  0.000000  0.693147  0.287682   \n1  0.000000  0.000000  0.000000  0.287682  0.287682  0.000000  0.287682   \n2  0.000000  0.693147  0.693147  0.000000  0.575364  0.000000  0.000000   \n3  0.693147  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n\n         저는       좋아요  \n0  0.000000  0.000000  \n1  0.000000  0.000000  \n2  0.000000  0.000000  \n3  0.693147  0.693147  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>과일이</th>\n      <th>길고</th>\n      <th>노란</th>\n      <th>먹고</th>\n      <th>바나나</th>\n      <th>사과</th>\n      <th>싶은</th>\n      <th>저는</th>\n      <th>좋아요</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.287682</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>0.287682</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.287682</td>\n      <td>0.287682</td>\n      <td>0.000000</td>\n      <td>0.287682</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>0.693147</td>\n      <td>0.000000</td>\n      <td>0.575364</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.693147</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.693147</td>\n      <td>0.693147</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF 행렬 출력\n",
    "\n",
    "result = []\n",
    "for i in range(N):\n",
    "  result.append([])\n",
    "  d = docs[i]\n",
    "  for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result[-1].append(tfidf(t,d))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns = vocab)\n",
    "tfidf_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T07:49:02.219283400Z",
     "start_time": "2024-01-17T07:49:02.185401Z"
    }
   },
   "id": "1523366cdc3046d6",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "사실 실제 TF-IDF 구현을 제공하고 있는 많은 머신 러닝 패키지들은 패키지마다 식이 조금씩 상이하지만\n",
    "위에서 배운 식과는 다른 조정된 식을 사용한다.\n",
    "\n",
    "**위의 기본적인 식을 바탕으로 한 구현의 문제점**\n",
    "\n",
    "- 전체 문서의 수 $n$이 4인데, $df(t)$의 값이 3인 경우 \n",
    "$df(t)$에 1이 더해지면서 log항의 분자와 분모의 값이 같아지게 된다.\n",
    "이는 $log$의 진수값이 1이 되면서 $idf(d,t)$의 값이 0이 됨을 의미.\n",
    "(식 : $idf(d,t)=log(n/(df(t)+1))=0$)\n",
    "IDF의 값이 0이라면 더 이상 가중치의 역할을 수행하지 못함.\n",
    "  <br/>\n",
    "- 사이킷런의 TF-IDF 구현체 또한 위의 식에서 조정된 식을 사용"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e31b578c873416ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. 사이킷런을 이용한 DTM과 TF-IDF 실습\n",
    "***\n",
    "\n",
    "BoW를 설명하며 배운 CountVectorizer를 사용하여 DTM 만들기"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87437fd02171d053"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "[('do', 0), ('know', 1), ('like', 2), ('love', 3), ('should', 4), ('want', 5), ('what', 6), ('you', 7), ('your', 8)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "\n",
    "# 각 단어와 맵핑된 인덱스 출력 (인덱스 기준 정렬 시킴)\n",
    "print(sorted(vector.vocabulary_.items()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T08:13:35.524380800Z",
     "start_time": "2024-01-17T08:13:34.766769400Z"
    }
   },
   "id": "819ac4bb46dbcbef",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 첫번째 열의 경우에는 0의 인덱스를 가진 do입니다.\n",
    "do는 세번째 문서에만 등장했기 때문에, 세번째 행에서만 1의 값을 가집니다.\n",
    " <br/>\n",
    "\n",
    "- 두번째 열의 경우에는 1의 인덱스를 가진 know입니다.\n",
    "know는 첫번째 문서에만 등장했으므로 첫번째 행에서만 1의 값을 가집니다.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e842a4e9b69a8929"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**사이킷런은 TF-IDF를 자동 계산해주는 TfidfVectorizer를 제공함**\n",
    "\n",
    "사이킷런의 TF-IDF는 위에서 배웠던 보편적인 TF-IDF 기본 식에서 조정된 식을 사용합니다.\n",
    "요약하자면, IDF의 로그항의 분자에 1을 더해주며, 로그항에 1을 더해주고, TF-IDF에 L2 정규화라는 방법으로 값을 조정하는 등의 차이로 TF-IDF가 가진 의도는 여전히 그대로 갖고 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "958337351efb0b5c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('do', 0), ('know', 1), ('like', 2), ('love', 3), ('should', 4), ('want', 5), ('what', 6), ('you', 7), ('your', 8)]\n",
      "['do', 'know', 'like', 'love', 'should', 'want', 'what', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "\n",
    "print(sorted(tfidfv.vocabulary_.items())) # 인덱스 기준 정렬\n",
    "print(sorted(tfidfv.vocabulary_))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T06:58:55.696918500Z",
     "start_time": "2024-01-12T06:58:55.692929Z"
    }
   },
   "id": "73b0cff1b22260d3",
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "        do      know      like      love   should      want     what  \\\n0  0.00000  0.467351  0.000000  0.467351  0.00000  0.467351  0.00000   \n1  0.00000  0.000000  0.795961  0.000000  0.00000  0.000000  0.00000   \n2  0.57735  0.000000  0.000000  0.000000  0.57735  0.000000  0.57735   \n\n        you      your  \n0  0.355432  0.467351  \n1  0.605349  0.000000  \n2  0.000000  0.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>do</th>\n      <th>know</th>\n      <th>like</th>\n      <th>love</th>\n      <th>should</th>\n      <th>want</th>\n      <th>what</th>\n      <th>you</th>\n      <th>your</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00000</td>\n      <td>0.467351</td>\n      <td>0.000000</td>\n      <td>0.467351</td>\n      <td>0.00000</td>\n      <td>0.467351</td>\n      <td>0.00000</td>\n      <td>0.355432</td>\n      <td>0.467351</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.795961</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.605349</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.57735</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.57735</td>\n      <td>0.000000</td>\n      <td>0.57735</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = tfidfv.transform(corpus).toarray()\n",
    "\n",
    "df = pd.DataFrame(arr, columns = sorted(tfidfv.vocabulary_))\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T06:59:00.522501200Z",
     "start_time": "2024-01-12T06:59:00.508117600Z"
    }
   },
   "id": "743a410fc3d056f4",
   "execution_count": 104
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
