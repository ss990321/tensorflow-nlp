{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# 03.언어 모델(Language Model)\n",
    "***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23b4bc368c154208"
  },
  {
   "cell_type": "markdown",
   "source": [
    "언어라는 현상을 모델링하고자 단어 시퀀스(문장)에 확률을 할당(assign)하는 모델\n",
    "\n",
    "언어 모델을 만드는 방법\n",
    "- 통계를 이용한 방법\n",
    "- 인공 신경망을 이용한 방법 (ex.GPT, BERT)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab9ff897dbeea66f"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f8c2ce8ba1779bff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## 03-01 언어 모델(Language Model)이란?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f444306494c61fcd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 언어 모델(Language Model)\n",
    "\n",
    "언어 모델은 단어 시퀀스에 확률을 할당(assign) 하는 일을 하는 모델입니다. 이를 조금 풀어서 쓰면, 언어 모델은 가장 자연스러운 단어 시퀀스를 찾아내는 모델입니다.\n",
    "단어 시퀀스에 확률을 할당하게 하기 위해서 가장 보편적으로 사용되는 방법은 언어 모델이 이전 단어들이 주어졌을 때 다음 단어를 예측하도록 하는 것입니다.\n",
    "\n",
    "언어 모델에 -ing를 붙인 언어 모델링(Language Modeling)은 주어진 단어들로부터 아직 모르는 단어를 예측하는 작업을 말합니다.\n",
    "즉, 언어 모델이 이전 단어들로부터 다음 단어를 예측하는 일은 언어 모델링입니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b93865769c3b278"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 단어 시퀀스의 확률 할당\n",
    "\n",
    "언어 모델은 확률을 통해 보다 적절한 문장을 판단합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3446d5282c582e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. 주어진 이전 단어들로부터 다음 단어 예측하기\n",
    "\n",
    "![수식](img.png)\n",
    "\n",
    "$\\prod$  $\\Rightarrow$ 부분곱 : 첫 항부터 마지막 항(제n항)까지 모두 곱하라는 뜻"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc640d125df7eae4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. 언어 모델의 간단한 직관\n",
    "\n",
    "앞에 어떤 단어들이 나왔는지 고려하여 후보가 될 수 있는 여러 단어들에 대해서 확률을 예측해보고 가장 높은 확률을 가진 단어를 선택합니다.\n",
    "앞에 어떤 단어들이 나왔는지 고려하여 후보가 될 수 있는 여러 단어들에 대해서 등장 확률을 추정하고 가장 높은 확률을 가진 단어를 선택합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c223b8032178129"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. 검색 엔진에서의 언어 모델의 예\n",
    "\n",
    "검색 엔진이 입력된 단어들의 나열에 대해서 다음 단어를 예측하는 언어 모델을 사용하고 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3ff05ba0c4b4f22"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8c2071b3cce1903e"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "99fdaf0370944567"
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## 03-02 통계적 언어 모델(Statistical Language Model, SLM)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad8dbbc08a06fe62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## 03-03 N-gram 언어 모델(N-gram Language Model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "321e8cc9f4b7221"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 코퍼스에서 카운트하지 못하는 경우의 감소.\n",
    "\n",
    "SLM의 한계는 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다는 점입니다.\n",
    "그리고 확률을 계산하고 싶은 문장이 길어질수록 갖고있는 코퍼스에서 그 문장이 존재하지 않을 가능성이 높습니다.\n",
    "다시 말하면 카운트할 수 없을 가능성이 높습니다. 그런데 다음과 같이 참고하는 단어들을 줄이면 카운트를 할 수 있을 가능성을 높일 수 있습니다.\n",
    "\n",
    "![수식](img_1.png)\n",
    "\n",
    "![수식](img_2.png)\n",
    "\n",
    "\n",
    "즉, 앞에서는 An adorable little boy가 나왔을 때 is가 나올 확률을 구하기 위해서는 An adorable little boy가 나온 횟수와 An adorable little boy is가 나온 횟수를 카운트해야만 했지만, 이제는 단어의 확률을 구하고자 기준 단어의 앞 단어를 전부 포함해서 카운트하는 것이 아니라, 앞 단어 중 임의의 개수만 포함해서 카운트하여 근사하자는 것입니다. 이렇게 하면 갖고 있는 코퍼스에서 해당 단어의 시퀀스를 카운트할 확률이 높아집니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3db5a388cb76189a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. N-gram\n",
    "\n",
    "이때 임의의 개수를 정하기 위한 기준을 정하는 것이 n-gram입니다. n-gram은 **n개의 연속적인 단어의 나열**을 의미합니다. \n",
    "갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주합니다.\n",
    "\n",
    "**uni**grams : an, adorable, little, boy, is, spreading, smiles\n",
    "**bi**grams : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles\n",
    "**tri**grams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles\n",
    "**4**-grams : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles\n",
    "\n",
    "n-gram을 사용할 때는 n이 1일 때는 유니그램(unigram), 2일 때는 바이그램(bigram), 3일 때는 트라이그램(trigram)이라고 명명하고 n이 4 이상일 때는 gram 앞에 그대로 숫자를 붙여서 명명합니다.\n",
    "출처에 따라서는 유니그램, 바이그램, 트라이그램 또한 각각 1-gram, 2-gram, 3-gram이라고 하기도 합니다.\n",
    "\n",
    "n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존합니다.\n",
    "n=4라고 한 4-gram을 이용한 언어 모델을 사용한다고 합시다. 이 경우, spreading 다음에 올 단어를 예측하는 것은 n-1에 해당되는 앞의 3개의 단어만을 고려합니다.\n",
    "\n",
    "![수식](img_3.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "587438a20db235c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. N-gram Language Model의 한계\n",
    "\n",
    "n-gram은 앞의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 생긴다는 점입니다.\n",
    "즉, 전체문장을 고려한 언어모델 보다는 정확도가 떨어질 수 밖에 없다.\n",
    "\n",
    "**n-gram 모델에 대한 한계점**\n",
    "+ 희소문제 : 카운트 확률을 높였지만 아직 존재함\n",
    "+ n을 선택하는 것은 trade-off 문제\n",
    "    - n을 크게 선택하면 실제 훈련 코퍼스에서 해당 n-gram을 카운트할 수 있는 확률은 적어지므로 희소 문제는 점점 심각해집니다.\n",
    "    - n이 커질수록 모델 사이즈가 커진다는 문제점도 있습니다.\n",
    "    - n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어집니다.\n",
    "    - trade-off 문제로 인해 정확도를 높이려면 n은 최대 5를 넘게 잡아서는 안 된다고 권장\n",
    "- trade-off란? : 모순적 관계를 의미"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2c3d0a0f76e18c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. 적용 분야(Domain)에 맞는 코퍼스의 수집\n",
    "\n",
    "어떤 분야인지, 어떤 어플리케이션인지에 따라서 특정 단어들의 확률 분포는 당연히 다릅니다.\n",
    "이 경우 언어 모델에 사용하는 코퍼스를 해당 도메인의 코퍼스를 사용한다면 당연히 언어 모델이 제대로 된 언어 생성을 할 가능성이 높아집니다.\n",
    "훈련에 사용된 도메인 코퍼스가 무엇이냐에 따라서 성능이 비약적으로 달라집니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7241f01dd3720a1b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. 인공 신경망을 이용한 언어 모델(Neural Network Based Language Model)\n",
    "\n",
    "n-gram 언어 모델에 대한 취약점을 완전히 해결하지는 못함\n",
    "대안으로 N-gram Language Model보다 대체적으로 성능이 우수한 인공 신경망을 이용한 언어 모델이 많이 사용되고 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "748f66ee9aedd1f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## 03-04 한국어에서의 언어 모델(Language Model for Korean Sentences)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fc37d44812cfca0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 한국어는 어순이 중요하지 않다.\n",
    "\n",
    "한국어에서는 어순이 중요하지 않습니다.\n",
    "그래서 이전 단어가 주어졌을때, 다음 단어가 나타날 확률을 구해야하는데 어순이 중요하지 않다는 것은\n",
    "다음 단어로 어떤 단어든 등장할 수 있다는 의미입니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd826d6c69849f32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 한국어는 교착어이다.\n",
    "\n",
    "띄어쓰기 단위인 어절 단위로 토큰화를 할 경우에는 문장에서 발생가능한 단어의 수가 굉장히 늘어납니다.\n",
    "\n",
    "한국어에는 조사가 있습니다.\n",
    "- 가령 '그녀'라는 단어 하나만 해도 그녀가, 그녀를, 그녀의, 그녀와, 그녀로, 그녀께서, 그녀처럼 등과 같이 다양한 경우가 존재합니다. 그렇기 때문에, 한국어에서는 **토큰화**를 통해 접사나 조사 등을 분리하는 것은 중요한 작업이 되기도 합니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd4d4ea7b2a25220"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7041ae00ecded19"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. 한국어는 띄어쓰기가 제대로 지켜지지 않는다.\n",
    "\n",
    "토큰이 제대로 분리 되지 않는채 훈련 데이터로 사용된다면 언어 모델은 제대로 동작하지 않습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fca32445209cf06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## 03-05 펄플렉서티(Perplexity, PPL)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b00d794b38a5ab0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 언어 모델의 평가 방법(Evaluation metric) : PPL\n",
    "\n",
    "펄플렉서티(perplexity)는 언어 모델을 평가하기 위한 평가 지표입니다. 보통 줄여서 PPL이 라고 표현합니다.\n",
    "- PPL : '헷갈리는 정도' - 수치가 낮을수록 언어모델의 성능이 좋다는 것을 의미함\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfe18f42568c6df8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 분기 계수(Branching factor)\n",
    "\n",
    "PPL은 선택할 수 있는 가능한 경우의 수를 의미하는 분기계수(branching factor)입니다.\n",
    "PPL은 이 언어 모델이 특정 시점에서 평균적으로 몇 개의 선택지를 가지고 고민하고 있는지를 의미합니다. \n",
    "\n",
    "가령, 언어 모델에 어떤 테스트 데이터을 주고 측정했더니 PPL이 10이 나왔다고 해봅시다.\n",
    "그렇다면 해당 언어 모델은 테스트 데이터에 대해서 다음 단어를 예측하는 모든 시점(time step)마다 평균 10개의 단어를 가지고 어떤 것이 정답인지 고민하고 있다고 볼 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b6587d7e98bd9e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. 기존 언어 모델 Vs. 인공 신경망을 이용한 언어 모델.\n",
    "\n",
    "인공 신경망을 이용한 언어 모델들은 대부분 n-gram을 이용한 언어 모델보다 더 좋은 성능 평가를 받았음을 확인할 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac70d160987dbd81"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
